---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


<h2>Alignment for Large Language Model</h2>
<ins>Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning</ins>
<br>
Jifan Zhang∗, Lalit Jain∗, <strong>Yang Guo∗</strong>, Jiayi Chen, Kuan Lok Zhou, Siddharth Suresh, Andrew Wagenmaker, Scott Sievert, Timothy Rogers, Kevin Jamieson, Robert Mankoff, Robert Nowak
<br>
Submitted to <em>Neural Information Processing Systems (Neurips) 2024 Datasets & Benchmarks</em>
<br>
<a href="https://arxiv.org/abs/2406.10522">[arXiv]</a> <a href="https://github.com/yguooo/cartoon-caption-generation">[code]</a> <a href="https://huggingface.co/datasets/yguooo/newyorker_caption_ranking">[dataset]</a>

<h2>Adversarial Robustness for Model Adaptation</h2>

<ins>Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection</ins>
<br>
Nils Palumbo*, <strong>Yang Guo*</strong>, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha
<br>
<em>International Conference on Machine Learning (ICML), 2024</em>
<br>
<a href="https://proceedings.mlr.press/v235/palumbo24a.html">[paper]</a> <a href="https://github.com/nilspalumbo/transduction-rejection">[code]</a>

<br><br>

<ins>Towards Evaluating the Robustness of Neural Networks Learned by Transduction</ins>
<br>
Jiefeng Chen, Xi Wu, <strong>Yang Guo</strong>, Yingyu Liang, and Somesh Jha
<br>
<em>International Conference on Learning Representations (ICLR), 2022</em>
<br>
<a href="https://openreview.net/forum?id=_5js_8uTrx1">[paper]</a><a href="https://github.com/jfc43/eval-transductive-robustness">[code]</a>

<br><br>

<ins>Best of Both Worlds: Towards Adversarial Robustness with Transduction and Rejection</ins>
<br>
Nils Palumbo*, <strong>Yang Guo*</strong>, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha
<br>
<em>ML Safety Workshop, Neural Information Processing Systems (Neurips) 2022</em>
<br>
<a href="https://openreview.net/pdf?id=F12SxTbzzp9">[paper]</a>



<h2> Memory-Efficient Tensor Decomposition </h2>

<ins>Low-rank tucker approximation of a tensor from streaming data</ins>
<br>
Yiming Sun, <strong>Yang Guo</strong>, Charlene Luo, Joel Tropp, and Madeleine Udell
<br>
<em>SIAM Journal on Mathematics of Data Science 2 (4), 1123-1150</em>
<br>
<a href="https://epubs.siam.org/doi/abs/10.1137/19M1257718?journalCode=sjmdaq">[paper]</a> <a href="https://github.com/udellgroup/tensorsketch">[code]</a>

<br><br>

<ins>Tensor random projection for low memory dimension reduction</ins>
<br>
<strong>Yang Guo*</strong>, Yiming Sun*, Joel Tropp, and Madeleine Udell
<br>
<em> Relational Representation Learning Workshop, Neural Information Processing Systems (Neurips), 2018 </em>
<br>
<a href="https://r2learning.github.io/assets/papers/CameraReadySubmission%2041.pdf">[paper]</a>


<h2> Manuscripts </h2>




<ins>Representation Bayesian Risk Decompositions and Multi-Source Domain Adaptation</ins>
<br>
Xi Wu, <strong>Yang Guo</strong>, Jiefeng Chen, Yingyu Liang, Somesh Jha, and Prasad Chalasani
<br>
<em> Manuscript </em>
<br>
<a href="https://arxiv.org/pdf/2004.10390">[arXiv]</a>


